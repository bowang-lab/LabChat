{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Required Libraries\n",
        "\n",
        "First, we import all the necessary libraries that are used in this notebook. These libraries provide functions and methods for web scraping, data processing, and generating embeddings and chat responses.\n"
      ],
      "metadata": {
        "id": "6LK5Rlb0vCpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment and run the code below to install dependencies\n",
        "# !pip install bs4 langchain openai chromadb unstructured tiktoken"
      ],
      "metadata": {
        "id": "MDf9Py38v4YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n"
      ],
      "metadata": {
        "id": "F-JMtVGNvEYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Helper Functions\n",
        "\n",
        "Two helper functions are defined:\n",
        "\n",
        "- `is_valid(url)`: Checks if a URL is valid.\n",
        "- `get_all_website_links(url)`: Retrieves all the links from a given website.\n"
      ],
      "metadata": {
        "id": "EO67vnEuvvCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid(url):\n",
        "    parsed = urlparse(url)\n",
        "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
        "\n",
        "def get_all_website_links(url):\n",
        "    urls = set()\n",
        "    domain_name = urlparse(url).netloc\n",
        "    try:\n",
        "        soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(e)\n",
        "        return urls\n",
        "\n",
        "    for a_tag in soup.findAll(\"a\"):\n",
        "        href = a_tag.attrs.get(\"href\")\n",
        "        if href == \"\" or href is None:\n",
        "            continue\n",
        "        href = urljoin(url, href)\n",
        "        parsed_href = urlparse(href)\n",
        "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
        "        if not is_valid(href):\n",
        "            continue\n",
        "        if domain_name not in href:\n",
        "            continue\n",
        "        urls.add(href)\n",
        "    return urls\n"
      ],
      "metadata": {
        "id": "sCzx6TyVu9yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Scraping\n",
        "\n",
        "We use the defined helper function `get_all_website_links(url)` to scrape all the links from the specified website. We then load the content from these links using `UnstructuredURLLoader`.\n"
      ],
      "metadata": {
        "id": "KvafePXwvzDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "links = get_all_website_links(\"https://wanglab.ml\") # Replace with your website of choice\n",
        "\n",
        "loader = UnstructuredURLLoader(urls=links)\n",
        "data = loader.load()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI0fur2Qvwk5",
        "outputId": "2e687f80-b992-42d4-d11c-55084fa1e870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Splitting\n",
        "\n",
        "The loaded data is split into smaller chunks using the `RecursiveCharacterTextSplitter` from the langchain library.\n"
      ],
      "metadata": {
        "id": "OZhZfr2TwRA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=())\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
        "    length_function=tiktoken_len,\n",
        "    separators=['\\n\\n', '\\n', ' ', '']\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(data)\n"
      ],
      "metadata": {
        "id": "z9Z7mqx_v0p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Additional data sources such as PDFs can also be added to the list of documents prior to creating a database for retrieval\n",
        "\n"
      ],
      "metadata": {
        "id": "zln5NdbCxnc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting the Dataset\n",
        "\n",
        "We then convert the split data into embeddings using the OpenAIEmbeddings from the langchain library. We then create a vector store using these embeddings.\n"
      ],
      "metadata": {
        "id": "0m89mTjawUAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = '' # Place your API key here\n",
        "persist_directory = '' # Place your target directory  here\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vector_store = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)\n",
        "vector_store.persist() # Only need to run this if running the code in a notebook\n"
      ],
      "metadata": {
        "id": "8-wS-I1ywSPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Model Mechanics\n",
        "\n",
        "Next, we set up the chat model mechanics. We start by creating an instance of `ChatOpenAI`. We then define a template for the chat prompt and use it to create a `PromptTemplate` instance. We also set up a question-answering chain with the help of `load_qa_chain`.\n"
      ],
      "metadata": {
        "id": "MzgxshMLwXgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0, streaming=True, model='gpt-3.5-turbo-16k')\n",
        "\n",
        "template = \"\"\"You are a bot, named [name of bot], having a conversation with a human.\n",
        "\n",
        "Background info: [desired background info]\n",
        "\n",
        "Given the following extracted parts of a long document and a question, create a final answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "{chat_history}\n",
        "Human: {human_input}\n",
        "[name of bot]:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"human_input\", \"context\"], template=template\n",
        ")\n",
        "\n",
        "chain = load_qa_chain(\n",
        "   llm, chain_type=\"stuff\", prompt=prompt\n",
        ")"
      ],
      "metadata": {
        "id": "_oztpgU7wVUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Response\n",
        "\n",
        "Finally, we generate a response from the chat model. We start by setting up a query and getting relevant documents related to the query from the vector store. We then pass these documents along with the query to the question-answering chain to generate a response.\n"
      ],
      "metadata": {
        "id": "WI2WJc4uwc7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Response\n",
        "\n",
        "query='what are some representative papers from the lab?'\n",
        "docs = vector_store.similarity_search(query)\n",
        "try:\n",
        "    chat_history\n",
        "except NameError:\n",
        "    chat_history = ''\n",
        "\n",
        "resp = chain({\"input_documents\": docs, \"human_input\": query,\"chat_history\":chat_history}, return_only_outputs=True)\n"
      ],
      "metadata": {
        "id": "1Repp96iwbG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, here's a simplified explanation in markdown:\n",
        "\n",
        "---\n",
        "\n",
        "## FastAPI Application for Chat Model\n",
        "\n",
        "This script sets up a FastAPI application to generate responses for chat queries using a language model. It's designed to be hosted on a cloud platform like Render.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "**Imported Libraries**: These include FastAPI for the web application, threading and queue for managing concurrent tasks, and various components from the `langchain` package for setting up the chat model.\n",
        "\n",
        "**ThreadedGenerator & ChainStreamHandler**: These are classes that manage the generation of responses in a separate thread, allowing responses to be streamed as they are generated.\n",
        "\n",
        "**llm_thread & chain Functions**: These functions work together to start a new thread for each chat request, generate the response from the language model, and send the response back to the client.\n",
        "\n",
        "**FastAPI Application**: This is the actual web application. It has a single endpoint, `/chain`, which accepts POST requests. Each request should have a JSON body with a `message` and `chat_history`. The endpoint responds with the generated chat response.\n",
        "\n",
        "### Deployment\n",
        "\n",
        "To deploy this application on Render:\n",
        "\n",
        "1. Create a GitHub repository and add this script, a `requirements.txt` file with the necessary Python packages, and your generated dataset.\n",
        "2. Set up a new Web Service on Render, linked to your GitHub repository. Render will automatically deploy your app and keep it synced with your repository.\n",
        "\n",
        "> Note: If your dataset is too large for GitHub or contains sensitive data, add it to your `.gitignore` file and upload it directly to Render."
      ],
      "metadata": {
        "id": "KGe2biUjy6Zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "import os\n",
        "import threading\n",
        "import queue\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import StreamingResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.callbacks.manager import AsyncCallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "persist_directory = 'data/j30'\n",
        "vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "template = \"\"\"You are a bot, named [name of bot], having a conversation with a human.\n",
        "\n",
        "Background info: [desired background info]\n",
        "\n",
        "Given the following extracted parts of a long document and a question, create a final answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "{chat_history}\n",
        "Human: {human_input}\n",
        "[name of bot]:\"\"\"\n",
        "\n",
        "pp = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"human_input\", \"context\"], template=template\n",
        ")\n",
        "\n",
        "class ThreadedGenerator:\n",
        "    def __init__(self):\n",
        "        self.queue = queue.Queue()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        item = self.queue.get()\n",
        "        if item is StopIteration: raise item\n",
        "        return item\n",
        "\n",
        "    def send(self, data):\n",
        "        self.queue.put(data)\n",
        "\n",
        "    def close(self):\n",
        "        self.queue.put(StopIteration)\n",
        "\n",
        "class ChainStreamHandler(StreamingStdOutCallbackHandler):\n",
        "    def __init__(self, gen):\n",
        "        super().__init__()\n",
        "        self.gen = gen\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs):\n",
        "        self.gen.send(token)\n",
        "\n",
        "def llm_thread(g, prompt, chat_history):\n",
        "    try:\n",
        "        llm = ChatOpenAI(temperature=0, streaming=True, model='gpt-3.5-turbo-16k', callback_manager=AsyncCallbackManager([ChainStreamHandler(g)]))\n",
        "        chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=pp)\n",
        "\n",
        "        docs = vector_store.similarity_search(prompt)\n",
        "        resp = chain({\"input_documents\": docs, \"human_input\": prompt, \"chat_history\": chat_history}, return_only_outputs=True)\n",
        "        ans = resp\n",
        "\n",
        "    finally:\n",
        "        g.close()\n",
        "\n",
        "def chain(prompt, chat_history):\n",
        "    g = ThreadedGenerator()\n",
        "    threading.Thread(target=llm_thread, args=(g, prompt, chat_history)).start()\n",
        "    return g\n",
        "\n",
        "class ChainRequest(BaseModel):\n",
        "    message: str\n",
        "    chat_history: str\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# CORS configuration\n",
        "origins = [\"*\"]\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.post(\"/chain\")\n",
        "async def _chain(request: ChainRequest):\n",
        "    gen = chain(request.message, request.chat_history)\n",
        "    return StreamingResponse(gen, media_type=\"text/plain\")\n",
        "```"
      ],
      "metadata": {
        "id": "FZGwrXrMzIJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying the FastAPI Application on Render\n",
        "\n",
        "Follow the steps below to deploy your FastAPI application on Render:\n",
        "\n",
        "1. **Create an account on Render**: If you haven't already, head over to [Render.com](https://render.com/) and sign up for a new account.\n",
        "\n",
        "2. **Create a new web service**: After logging in, click on the \"New+\" dropdown menu on the dashboard. Select \"Web Service\" from the dropdown.\n",
        "\n",
        "3. **Link your GitHub repository**: Connect your GitHub account and select the repository where you saved the FastAPI script as `app.py`. This repository should also contain the `requirements.txt` file and the dataset.\n",
        "\n",
        "4. **Set the environment**: Make sure the environment is set to Python 3.\n",
        "\n",
        "5. **Specify the build and start commands**:\n",
        "    - In the \"Build Command\" field, enter `pip install -r requirements.txt`.\n",
        "    - In the \"Start Command\" field, enter `uvicorn main:app --host 0.0.0.0 --port $PORT`.\n",
        "\n",
        "6. **Set the environment variables**: Navigate to the \"Environment\" tab and set the following keys:\n",
        "    - Key: `PYTHON_VERSION`, Value: `3.10.0`\n",
        "    - Key: `OPENAI_API_KEY`, Value: [your open_api_key]\n",
        "\n",
        "Once you've entered these settings, click \"Create\" to create the web service. Render will automatically build and deploy your application, and it will remain synced with your GitHub repository.\n",
        "\n",
        "> Note: Ensure that your dataset is included in the repository if it's not too large. If it's too large for GitHub, you can upload it directly to Render via the shell after deployment.\n"
      ],
      "metadata": {
        "id": "FB-6H9VF0PJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `requirements.txt` File\n",
        "\n",
        "The `requirements.txt` file lists the Python packages that your project depends on. You can automatically install these packages using pip. Here's what the `requirements.txt` file looks like for this project:\n",
        "\n",
        "```plaintext\n",
        "fastapi[all]\n",
        "langchain==0.0.205\n",
        "openai\n",
        "chromadb\n",
        "Gunicorn\n",
        "nest_asyncio\n",
        "```"
      ],
      "metadata": {
        "id": "6wk2mlzo1LON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the FastAPI Application\n",
        "\n",
        "You can test the application by sending POST requests to the `/chain` endpoint. In this example, we're using the `requests` library in Python to send these requests. Note that we're managing the chat history on the client side. This means that we append the chat history to new messages to ensure they remain in context.\n",
        "\n",
        "Here is an example script to test the application:\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "# Initialize the chat message and history\n",
        "data = {\n",
        "    \"message\": \"who is rex ma\",\n",
        "    \"chat_history\": \"\"\n",
        "}\n",
        "\n",
        "# Send the first request and update the chat history\n",
        "response = requests.post(\"[your render link]\", json=data)\n",
        "data['chat_history'] += 'Human: ' + data['message'] + '\\n' + '[name of bot]: ' + response.text + '\\n'\n",
        "\n",
        "# Print the response\n",
        "print(response.text)\n",
        "```\n",
        "\n",
        "The output should be something like:\n",
        "\n",
        "```\n",
        "Rex is a research student pursuing a PhD at the lab. His research area focuses on multi-modality integration in healthcare and biology using deep learning. He holds a BASc in Computer Engineering from the University of Toronto.\n",
        "```\n",
        "\n",
        "You can continue the conversation by sending more requests:\n",
        "\n",
        "```python\n",
        "# Set a new message\n",
        "data['message'] = 'wait, is he an engineer?'\n",
        "\n",
        "# Send the second request and update the chat history\n",
        "response = requests.post(\"[your render link]\", json=data)\n",
        "data['chat_history'] += 'Human: ' + data['message'] + '\\n' + '[name of bot]: ' + response.text + '\\n'\n",
        "\n",
        "# Print the response\n",
        "print(response.text)\n",
        "```\n",
        "\n",
        "The output should be something like:\n",
        "\n",
        "```\n",
        "Yes, Rex Ma has a BASc in Computer Engineering from the University of Toronto.\n",
        "```\n",
        "\n",
        "As you can see, the application is able to generate relevant responses based on the chat history.\n"
      ],
      "metadata": {
        "id": "X62_bN-q0uns"
      }
    }
  ]
}